{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darveenvijayan/nanoGPT/blob/master/nanoGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup NB"
      ],
      "metadata": {
        "id": "CGhruaYfYM2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torch numpy transformers datasets tiktoken wandb tqdm"
      ],
      "metadata": {
        "id": "y8RO_B8HwKm2",
        "outputId": "bda8f5d3-e2ed-4374-97af-f1f5de636fc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.15.8-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.6)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.29.2-py2.py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.6/215.6 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=2839f39a11cea2b084817d3b343239e83de5122c52e05b4a7822a51a5d38db6f\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: tokenizers, safetensors, pathtools, xxhash, smmap, setproctitle, sentry-sdk, docker-pycreds, dill, tiktoken, multiprocess, huggingface-hub, gitdb, transformers, GitPython, wandb, datasets\n",
            "Successfully installed GitPython-3.1.32 datasets-2.14.4 dill-0.3.7 docker-pycreds-0.4.0 gitdb-4.0.10 huggingface-hub-0.16.4 multiprocess-0.70.15 pathtools-0.1.2 safetensors-0.3.2 sentry-sdk-1.29.2 setproctitle-1.3.2 smmap-5.0.0 tiktoken-0.4.0 tokenizers-0.13.3 transformers-4.31.0 wandb-0.15.8 xxhash-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get repo\n",
        "\n",
        "!git clone https://github.com/darveenvijayan/nanoGPT\n",
        "\n",
        "%cd nanoGPT"
      ],
      "metadata": {
        "id": "bS8-BUpuXP_x",
        "outputId": "9003adcb-c558-40e9-d221-780ad1108cea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 655, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 655 (delta 2), reused 0 (delta 0), pack-reused 649\u001b[K\n",
            "Receiving objects: 100% (655/655), 957.67 KiB | 6.99 MiB/s, done.\n",
            "Resolving deltas: 100% (375/375), done.\n",
            "/content/nanoGPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Data"
      ],
      "metadata": {
        "id": "WcCA9_9UYJCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python data/shakespeare_char/prepare.py # This creates a train.bin and val.bin in that data directory."
      ],
      "metadata": {
        "id": "5r9u9aOEXP8Y",
        "outputId": "08f4cf73-69b2-4feb-88ba-4a7288f87b1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,394\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train a baby GPT based on config in config/train_shakespeare_char.py"
      ],
      "metadata": {
        "id": "0yD-oMxgY1uh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py config/train_shakespeare_char.py\n",
        "\n",
        "# we're training a GPT with a context size of up to 256 characters,\n",
        "# 384 feature channels, and it is a 6-layer Transformer\n",
        "# with 6 heads in each layer."
      ],
      "metadata": {
        "id": "XwugbEUbXP6B",
        "outputId": "5fdc5fe8-9a7b-46d0-dfa2-a863d80384da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 10.65M\n",
            "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 4.2874, val loss 4.2823\n",
            "[2023-08-11 02:29:54,108] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-08-11 02:29:54,857] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-08-11 02:29:55,918] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-08-11 02:29:56,191] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-08-11 02:29:56,608] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-08-11 02:29:56,881] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-08-11 02:29:57,310] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-08-11 02:29:57,586] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-08-11 02:29:58,014] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-08-11 02:29:58,283] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-08-11 02:29:58,700] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-08-11 02:29:59,132] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "iter 0: loss 4.2649, time 36539.44ms, mfu -100.00%\n",
            "iter 10: loss 3.2438, time 104.33ms, mfu 3.57%\n",
            "iter 20: loss 2.7899, time 104.43ms, mfu 3.57%\n",
            "iter 30: loss 2.6383, time 104.59ms, mfu 3.57%\n",
            "iter 40: loss 2.5763, time 104.76ms, mfu 3.57%\n",
            "iter 50: loss 2.5261, time 104.05ms, mfu 3.57%\n",
            "iter 60: loss 2.5136, time 104.99ms, mfu 3.57%\n",
            "iter 70: loss 2.4921, time 104.81ms, mfu 3.57%\n",
            "iter 80: loss 2.4932, time 104.95ms, mfu 3.57%\n",
            "iter 90: loss 2.4696, time 107.90ms, mfu 3.55%\n",
            "iter 100: loss 2.4526, time 107.12ms, mfu 3.55%\n",
            "iter 110: loss 2.4543, time 105.85ms, mfu 3.54%\n",
            "iter 120: loss 2.4223, time 107.03ms, mfu 3.54%\n",
            "iter 130: loss 2.4059, time 106.42ms, mfu 3.53%\n",
            "iter 140: loss 2.3925, time 111.20ms, mfu 3.52%\n",
            "iter 150: loss 2.4098, time 107.82ms, mfu 3.51%\n",
            "iter 160: loss 2.3675, time 105.24ms, mfu 3.51%\n",
            "iter 170: loss 2.3382, time 105.53ms, mfu 3.51%\n",
            "iter 180: loss 2.3011, time 107.15ms, mfu 3.51%\n",
            "iter 190: loss 2.2278, time 107.33ms, mfu 3.51%\n",
            "iter 200: loss 2.2004, time 108.46ms, mfu 3.50%\n",
            "iter 210: loss 2.1244, time 113.09ms, mfu 3.48%\n",
            "iter 220: loss 2.1338, time 106.47ms, mfu 3.48%\n",
            "iter 230: loss 2.0709, time 107.82ms, mfu 3.48%\n",
            "iter 240: loss 2.0742, time 112.26ms, mfu 3.46%\n",
            "step 250: train loss 1.9616, val loss 2.0647\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 250: loss 2.0277, time 17721.05ms, mfu 3.12%\n",
            "iter 260: loss 1.9685, time 109.37ms, mfu 3.15%\n",
            "iter 270: loss 1.9776, time 110.23ms, mfu 3.17%\n",
            "iter 280: loss 1.9798, time 106.34ms, mfu 3.20%\n",
            "iter 290: loss 1.9237, time 113.82ms, mfu 3.21%\n",
            "iter 300: loss 1.8944, time 108.79ms, mfu 3.23%\n",
            "iter 310: loss 1.8637, time 111.18ms, mfu 3.24%\n",
            "iter 320: loss 1.8569, time 109.96ms, mfu 3.26%\n",
            "iter 330: loss 1.8088, time 109.14ms, mfu 3.27%\n",
            "iter 340: loss 1.7812, time 120.93ms, mfu 3.26%\n",
            "iter 350: loss 1.8272, time 111.26ms, mfu 3.26%\n",
            "iter 360: loss 1.7745, time 112.07ms, mfu 3.27%\n",
            "iter 370: loss 1.7414, time 113.12ms, mfu 3.27%\n",
            "iter 380: loss 1.7304, time 112.98ms, mfu 3.28%\n",
            "iter 390: loss 1.7372, time 112.01ms, mfu 3.28%\n",
            "iter 400: loss 1.7640, time 111.69ms, mfu 3.29%\n",
            "iter 410: loss 1.6959, time 112.21ms, mfu 3.29%\n",
            "iter 420: loss 1.7088, time 111.11ms, mfu 3.30%\n",
            "iter 430: loss 1.6815, time 114.59ms, mfu 3.29%\n",
            "iter 440: loss 1.6462, time 112.76ms, mfu 3.29%\n",
            "iter 450: loss 1.6511, time 112.57ms, mfu 3.29%\n",
            "iter 460: loss 1.6024, time 111.55ms, mfu 3.30%\n",
            "iter 470: loss 1.6554, time 112.50ms, mfu 3.30%\n",
            "iter 480: loss 1.6165, time 111.26ms, mfu 3.31%\n",
            "iter 490: loss 1.6016, time 116.00ms, mfu 3.30%\n",
            "step 500: train loss 1.5285, val loss 1.7362\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 500: loss 1.6016, time 11821.55ms, mfu 2.97%\n",
            "iter 510: loss 1.6162, time 112.21ms, mfu 3.00%\n",
            "iter 520: loss 1.6020, time 111.61ms, mfu 3.04%\n",
            "iter 530: loss 1.5657, time 112.25ms, mfu 3.07%\n",
            "iter 540: loss 1.6203, time 110.70ms, mfu 3.10%\n",
            "iter 550: loss 1.5671, time 110.99ms, mfu 3.12%\n",
            "iter 560: loss 1.5651, time 111.46ms, mfu 3.14%\n",
            "iter 570: loss 1.5745, time 109.78ms, mfu 3.17%\n",
            "iter 580: loss 1.5401, time 111.28ms, mfu 3.19%\n",
            "iter 590: loss 1.5016, time 110.83ms, mfu 3.20%\n",
            "iter 600: loss 1.5180, time 111.04ms, mfu 3.22%\n",
            "iter 610: loss 1.5552, time 110.78ms, mfu 3.23%\n",
            "iter 620: loss 1.5292, time 111.84ms, mfu 3.24%\n",
            "iter 630: loss 1.5179, time 113.82ms, mfu 3.25%\n",
            "iter 640: loss 1.4779, time 111.14ms, mfu 3.26%\n",
            "iter 650: loss 1.5043, time 112.76ms, mfu 3.26%\n",
            "iter 660: loss 1.5145, time 110.68ms, mfu 3.27%\n",
            "iter 670: loss 1.4491, time 111.51ms, mfu 3.28%\n",
            "iter 680: loss 1.5118, time 111.14ms, mfu 3.29%\n",
            "iter 690: loss 1.4611, time 112.08ms, mfu 3.29%\n",
            "iter 700: loss 1.4802, time 111.27ms, mfu 3.30%\n",
            "iter 710: loss 1.4568, time 111.36ms, mfu 3.30%\n",
            "iter 720: loss 1.4481, time 109.68ms, mfu 3.31%\n",
            "iter 730: loss 1.4214, time 111.21ms, mfu 3.32%\n",
            "iter 740: loss 1.4311, time 111.50ms, mfu 3.32%\n",
            "step 750: train loss 1.3611, val loss 1.5957\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 750: loss 1.4226, time 11500.38ms, mfu 2.99%\n",
            "iter 760: loss 1.4461, time 109.74ms, mfu 3.03%\n",
            "iter 770: loss 1.4283, time 112.88ms, mfu 3.06%\n",
            "iter 780: loss 1.4123, time 112.74ms, mfu 3.08%\n",
            "iter 790: loss 1.4221, time 111.05ms, mfu 3.11%\n",
            "iter 800: loss 1.4286, time 110.44ms, mfu 3.14%\n",
            "iter 810: loss 1.4068, time 113.25ms, mfu 3.15%\n",
            "iter 820: loss 1.4105, time 112.42ms, mfu 3.17%\n",
            "iter 830: loss 1.3939, time 110.91ms, mfu 3.19%\n",
            "iter 840: loss 1.4056, time 112.05ms, mfu 3.20%\n",
            "iter 850: loss 1.3911, time 111.57ms, mfu 3.21%\n",
            "iter 860: loss 1.4010, time 111.11ms, mfu 3.23%\n",
            "iter 870: loss 1.4061, time 110.71ms, mfu 3.24%\n",
            "iter 880: loss 1.3758, time 112.24ms, mfu 3.25%\n",
            "iter 890: loss 1.3920, time 112.13ms, mfu 3.26%\n",
            "iter 900: loss 1.3728, time 112.41ms, mfu 3.26%\n",
            "iter 910: loss 1.3245, time 109.98ms, mfu 3.28%\n",
            "iter 920: loss 1.3671, time 111.76ms, mfu 3.28%\n",
            "iter 930: loss 1.3670, time 111.34ms, mfu 3.29%\n",
            "iter 940: loss 1.3524, time 112.92ms, mfu 3.29%\n",
            "iter 950: loss 1.3538, time 111.01ms, mfu 3.30%\n",
            "iter 960: loss 1.3668, time 111.43ms, mfu 3.30%\n",
            "iter 970: loss 1.3636, time 111.72ms, mfu 3.30%\n",
            "iter 980: loss 1.3566, time 111.84ms, mfu 3.31%\n",
            "iter 990: loss 1.3400, time 111.04ms, mfu 3.31%\n",
            "step 1000: train loss 1.2762, val loss 1.5256\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.3415, time 11799.19ms, mfu 2.98%\n",
            "iter 1010: loss 1.3431, time 111.53ms, mfu 3.02%\n",
            "iter 1020: loss 1.3133, time 109.81ms, mfu 3.06%\n",
            "iter 1030: loss 1.3367, time 111.41ms, mfu 3.09%\n",
            "iter 1040: loss 1.3645, time 111.31ms, mfu 3.11%\n",
            "iter 1050: loss 1.2936, time 108.83ms, mfu 3.14%\n",
            "iter 1060: loss 1.3426, time 110.11ms, mfu 3.17%\n",
            "iter 1070: loss 1.3305, time 111.51ms, mfu 3.18%\n",
            "iter 1080: loss 1.3386, time 112.33ms, mfu 3.20%\n",
            "iter 1090: loss 1.3539, time 110.84ms, mfu 3.21%\n",
            "iter 1100: loss 1.3168, time 111.66ms, mfu 3.23%\n",
            "iter 1110: loss 1.3008, time 113.10ms, mfu 3.23%\n",
            "iter 1120: loss 1.3026, time 111.39ms, mfu 3.24%\n",
            "iter 1130: loss 1.3015, time 112.77ms, mfu 3.25%\n",
            "iter 1140: loss 1.2992, time 110.70ms, mfu 3.26%\n",
            "iter 1150: loss 1.3126, time 110.68ms, mfu 3.27%\n",
            "iter 1160: loss 1.3269, time 112.73ms, mfu 3.28%\n",
            "iter 1170: loss 1.3064, time 111.23ms, mfu 3.28%\n",
            "iter 1180: loss 1.3226, time 110.90ms, mfu 3.29%\n",
            "iter 1190: loss 1.2668, time 113.45ms, mfu 3.29%\n",
            "iter 1200: loss 1.2964, time 111.92ms, mfu 3.29%\n",
            "iter 1210: loss 1.2739, time 113.83ms, mfu 3.29%\n",
            "iter 1220: loss 1.3009, time 111.70ms, mfu 3.30%\n",
            "iter 1230: loss 1.2977, time 110.91ms, mfu 3.30%\n",
            "iter 1240: loss 1.3042, time 109.89ms, mfu 3.31%\n",
            "step 1250: train loss 1.2079, val loss 1.4969\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1250: loss 1.2753, time 11684.28ms, mfu 2.98%\n",
            "iter 1260: loss 1.2867, time 111.18ms, mfu 3.02%\n",
            "iter 1270: loss 1.2715, time 111.49ms, mfu 3.05%\n",
            "iter 1280: loss 1.2605, time 111.94ms, mfu 3.08%\n",
            "iter 1290: loss 1.2806, time 111.98ms, mfu 3.10%\n",
            "iter 1300: loss 1.2990, time 112.42ms, mfu 3.13%\n",
            "iter 1310: loss 1.2402, time 112.26ms, mfu 3.15%\n",
            "iter 1320: loss 1.3072, time 111.40ms, mfu 3.17%\n",
            "iter 1330: loss 1.2705, time 110.06ms, mfu 3.19%\n",
            "iter 1340: loss 1.2992, time 112.48ms, mfu 3.20%\n",
            "iter 1350: loss 1.2529, time 110.53ms, mfu 3.22%\n",
            "iter 1360: loss 1.2679, time 112.18ms, mfu 3.23%\n",
            "iter 1370: loss 1.2591, time 112.41ms, mfu 3.24%\n",
            "iter 1380: loss 1.2695, time 111.33ms, mfu 3.25%\n",
            "iter 1390: loss 1.2550, time 111.63ms, mfu 3.26%\n",
            "iter 1400: loss 1.2619, time 112.01ms, mfu 3.26%\n",
            "iter 1410: loss 1.2513, time 112.06ms, mfu 3.27%\n",
            "iter 1420: loss 1.2738, time 111.90ms, mfu 3.28%\n",
            "iter 1430: loss 1.2424, time 113.00ms, mfu 3.28%\n",
            "iter 1440: loss 1.2576, time 112.77ms, mfu 3.28%\n",
            "iter 1450: loss 1.2388, time 112.18ms, mfu 3.28%\n",
            "iter 1460: loss 1.2448, time 111.35ms, mfu 3.29%\n",
            "iter 1470: loss 1.2223, time 111.97ms, mfu 3.29%\n",
            "iter 1480: loss 1.2091, time 111.78ms, mfu 3.30%\n",
            "iter 1490: loss 1.2391, time 112.74ms, mfu 3.30%\n",
            "step 1500: train loss 1.1533, val loss 1.4693\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1500: loss 1.1903, time 11670.45ms, mfu 2.97%\n",
            "iter 1510: loss 1.2334, time 113.23ms, mfu 3.00%\n",
            "iter 1520: loss 1.2242, time 112.70ms, mfu 3.03%\n",
            "iter 1530: loss 1.2554, time 112.91ms, mfu 3.06%\n",
            "iter 1540: loss 1.1952, time 111.16ms, mfu 3.09%\n",
            "iter 1550: loss 1.2376, time 112.22ms, mfu 3.11%\n",
            "iter 1560: loss 1.2124, time 110.54ms, mfu 3.14%\n",
            "iter 1570: loss 1.2289, time 110.00ms, mfu 3.16%\n",
            "iter 1580: loss 1.2126, time 110.86ms, mfu 3.18%\n",
            "iter 1590: loss 1.1923, time 111.55ms, mfu 3.20%\n",
            "iter 1600: loss 1.2019, time 110.18ms, mfu 3.22%\n",
            "iter 1610: loss 1.2397, time 111.60ms, mfu 3.23%\n",
            "iter 1620: loss 1.1842, time 110.40ms, mfu 3.24%\n",
            "iter 1630: loss 1.2066, time 110.95ms, mfu 3.26%\n",
            "iter 1640: loss 1.2057, time 112.25ms, mfu 3.26%\n",
            "iter 1650: loss 1.1848, time 112.72ms, mfu 3.27%\n",
            "iter 1660: loss 1.2201, time 112.96ms, mfu 3.27%\n",
            "iter 1670: loss 1.2026, time 113.65ms, mfu 3.27%\n",
            "iter 1680: loss 1.2037, time 111.01ms, mfu 3.28%\n",
            "iter 1690: loss 1.2076, time 110.62ms, mfu 3.29%\n",
            "iter 1700: loss 1.1913, time 110.30ms, mfu 3.30%\n",
            "iter 1710: loss 1.1841, time 111.86ms, mfu 3.30%\n",
            "iter 1720: loss 1.1854, time 110.61ms, mfu 3.31%\n",
            "iter 1730: loss 1.1986, time 111.47ms, mfu 3.31%\n",
            "iter 1740: loss 1.1763, time 110.46ms, mfu 3.32%\n",
            "step 1750: train loss 1.1028, val loss 1.4603\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1750: loss 1.1805, time 12094.89ms, mfu 2.99%\n",
            "iter 1760: loss 1.1936, time 112.00ms, mfu 3.02%\n",
            "iter 1770: loss 1.1979, time 113.06ms, mfu 3.05%\n",
            "iter 1780: loss 1.1955, time 111.35ms, mfu 3.08%\n",
            "iter 1790: loss 1.1912, time 110.55ms, mfu 3.11%\n",
            "iter 1800: loss 1.1752, time 111.32ms, mfu 3.13%\n",
            "iter 1810: loss 1.1647, time 110.15ms, mfu 3.16%\n",
            "iter 1820: loss 1.1709, time 118.20ms, mfu 3.16%\n",
            "iter 1830: loss 1.1703, time 117.39ms, mfu 3.16%\n",
            "iter 1840: loss 1.1592, time 119.90ms, mfu 3.15%\n",
            "iter 1850: loss 1.1671, time 113.98ms, mfu 3.17%\n",
            "iter 1860: loss 1.1751, time 112.22ms, mfu 3.18%\n",
            "iter 1870: loss 1.1444, time 111.42ms, mfu 3.20%\n",
            "iter 1880: loss 1.1820, time 115.71ms, mfu 3.20%\n",
            "iter 1890: loss 1.1850, time 110.05ms, mfu 3.22%\n",
            "iter 1900: loss 1.1317, time 109.73ms, mfu 3.24%\n",
            "iter 1910: loss 1.1653, time 111.65ms, mfu 3.25%\n",
            "iter 1920: loss 1.1680, time 111.31ms, mfu 3.26%\n",
            "iter 1930: loss 1.1419, time 110.27ms, mfu 3.27%\n",
            "iter 1940: loss 1.1259, time 110.11ms, mfu 3.28%\n",
            "iter 1950: loss 1.1378, time 111.86ms, mfu 3.29%\n",
            "iter 1960: loss 1.1573, time 110.18ms, mfu 3.29%\n",
            "iter 1970: loss 1.1574, time 111.83ms, mfu 3.30%\n",
            "iter 1980: loss 1.1518, time 111.69ms, mfu 3.30%\n",
            "iter 1990: loss 1.1529, time 110.69ms, mfu 3.31%\n",
            "step 2000: train loss 1.0560, val loss 1.4723\n",
            "iter 2000: loss 1.1313, time 11400.25ms, mfu 2.98%\n",
            "iter 2010: loss 1.1371, time 110.96ms, mfu 3.02%\n",
            "iter 2020: loss 1.1267, time 107.87ms, mfu 3.06%\n",
            "iter 2030: loss 1.1575, time 112.16ms, mfu 3.09%\n",
            "iter 2040: loss 1.1369, time 111.99ms, mfu 3.11%\n",
            "iter 2050: loss 1.1200, time 110.59ms, mfu 3.14%\n",
            "iter 2060: loss 1.1054, time 113.40ms, mfu 3.15%\n",
            "iter 2070: loss 1.1241, time 112.98ms, mfu 3.17%\n",
            "iter 2080: loss 1.1240, time 111.85ms, mfu 3.18%\n",
            "iter 2090: loss 1.1352, time 109.84ms, mfu 3.20%\n",
            "iter 2100: loss 1.1349, time 110.66ms, mfu 3.22%\n",
            "iter 2110: loss 1.1359, time 111.29ms, mfu 3.23%\n",
            "iter 2120: loss 1.1305, time 110.59ms, mfu 3.25%\n",
            "iter 2130: loss 1.1397, time 110.31ms, mfu 3.26%\n",
            "iter 2140: loss 1.1365, time 111.26ms, mfu 3.27%\n",
            "iter 2150: loss 1.1284, time 111.80ms, mfu 3.28%\n",
            "iter 2160: loss 1.1414, time 111.61ms, mfu 3.28%\n",
            "iter 2170: loss 1.1367, time 111.80ms, mfu 3.29%\n",
            "iter 2180: loss 1.1187, time 111.72ms, mfu 3.29%\n",
            "iter 2190: loss 1.1095, time 112.65ms, mfu 3.29%\n",
            "iter 2200: loss 1.1194, time 112.37ms, mfu 3.30%\n",
            "iter 2210: loss 1.1110, time 112.15ms, mfu 3.30%\n",
            "iter 2220: loss 1.1232, time 113.52ms, mfu 3.30%\n",
            "iter 2230: loss 1.1231, time 110.85ms, mfu 3.30%\n",
            "iter 2240: loss 1.1356, time 112.20ms, mfu 3.31%\n",
            "step 2250: train loss 1.0100, val loss 1.4719\n",
            "iter 2250: loss 1.1040, time 11419.95ms, mfu 2.98%\n",
            "iter 2260: loss 1.1040, time 114.90ms, mfu 3.00%\n",
            "iter 2270: loss 1.1425, time 112.49ms, mfu 3.04%\n",
            "iter 2280: loss 1.0969, time 110.87ms, mfu 3.07%\n",
            "iter 2290: loss 1.1535, time 112.25ms, mfu 3.09%\n",
            "iter 2300: loss 1.1172, time 112.09ms, mfu 3.12%\n",
            "iter 2310: loss 1.0925, time 118.18ms, mfu 3.12%\n",
            "iter 2320: loss 1.0992, time 112.62ms, mfu 3.14%\n",
            "iter 2330: loss 1.1028, time 110.74ms, mfu 3.16%\n",
            "iter 2340: loss 1.1236, time 111.52ms, mfu 3.18%\n",
            "iter 2350: loss 1.1000, time 111.49ms, mfu 3.20%\n",
            "iter 2360: loss 1.1014, time 110.81ms, mfu 3.21%\n",
            "iter 2370: loss 1.0974, time 111.38ms, mfu 3.23%\n",
            "iter 2380: loss 1.0846, time 112.04ms, mfu 3.24%\n",
            "iter 2390: loss 1.0864, time 110.01ms, mfu 3.25%\n",
            "iter 2400: loss 1.0765, time 111.59ms, mfu 3.26%\n",
            "iter 2410: loss 1.0720, time 111.93ms, mfu 3.27%\n",
            "iter 2420: loss 1.0792, time 110.92ms, mfu 3.28%\n",
            "iter 2430: loss 1.0588, time 109.87ms, mfu 3.29%\n",
            "iter 2440: loss 1.0537, time 110.39ms, mfu 3.30%\n",
            "iter 2450: loss 1.0758, time 111.18ms, mfu 3.30%\n",
            "iter 2460: loss 1.0872, time 111.13ms, mfu 3.31%\n",
            "iter 2470: loss 1.0853, time 110.13ms, mfu 3.31%\n",
            "iter 2480: loss 1.0859, time 112.08ms, mfu 3.32%\n",
            "iter 2490: loss 1.0610, time 110.34ms, mfu 3.32%\n",
            "step 2500: train loss 0.9590, val loss 1.4886\n",
            "iter 2500: loss 1.0800, time 11464.70ms, mfu 2.99%\n",
            "iter 2510: loss 1.0775, time 112.14ms, mfu 3.03%\n",
            "iter 2520: loss 1.0480, time 111.67ms, mfu 3.06%\n",
            "iter 2530: loss 1.0608, time 111.64ms, mfu 3.08%\n",
            "iter 2540: loss 1.0559, time 111.79ms, mfu 3.11%\n",
            "iter 2550: loss 1.0655, time 112.80ms, mfu 3.13%\n",
            "iter 2560: loss 1.0615, time 111.58ms, mfu 3.15%\n",
            "iter 2570: loss 1.0752, time 111.91ms, mfu 3.17%\n",
            "iter 2580: loss 1.0818, time 112.36ms, mfu 3.18%\n",
            "iter 2590: loss 1.0710, time 111.72ms, mfu 3.20%\n",
            "iter 2600: loss 1.0741, time 110.29ms, mfu 3.22%\n",
            "iter 2610: loss 1.0476, time 112.38ms, mfu 3.23%\n",
            "iter 2620: loss 1.0413, time 112.44ms, mfu 3.23%\n",
            "iter 2630: loss 1.0266, time 112.04ms, mfu 3.24%\n",
            "iter 2640: loss 1.0461, time 114.75ms, mfu 3.24%\n",
            "iter 2650: loss 1.0662, time 111.64ms, mfu 3.25%\n",
            "iter 2660: loss 1.0466, time 111.45ms, mfu 3.26%\n",
            "iter 2670: loss 1.0205, time 110.29ms, mfu 3.27%\n",
            "iter 2680: loss 1.0463, time 110.96ms, mfu 3.28%\n",
            "iter 2690: loss 1.0574, time 112.80ms, mfu 3.28%\n",
            "iter 2700: loss 1.0213, time 112.25ms, mfu 3.29%\n",
            "iter 2710: loss 1.0412, time 110.62ms, mfu 3.30%\n",
            "iter 2720: loss 1.0447, time 110.88ms, mfu 3.30%\n",
            "iter 2730: loss 1.0544, time 111.29ms, mfu 3.31%\n",
            "iter 2740: loss 1.0265, time 109.28ms, mfu 3.32%\n",
            "step 2750: train loss 0.9135, val loss 1.5104\n",
            "iter 2750: loss 1.0351, time 11366.48ms, mfu 2.99%\n",
            "iter 2760: loss 1.0271, time 114.35ms, mfu 3.02%\n",
            "iter 2770: loss 1.0300, time 110.50ms, mfu 3.05%\n",
            "iter 2780: loss 1.0212, time 112.62ms, mfu 3.08%\n",
            "iter 2790: loss 1.0406, time 111.71ms, mfu 3.10%\n",
            "iter 2800: loss 1.0119, time 111.11ms, mfu 3.13%\n",
            "iter 2810: loss 1.0451, time 111.78ms, mfu 3.15%\n",
            "iter 2820: loss 1.0255, time 110.47ms, mfu 3.17%\n",
            "iter 2830: loss 1.0377, time 111.52ms, mfu 3.19%\n",
            "iter 2840: loss 1.0019, time 111.21ms, mfu 3.20%\n",
            "iter 2850: loss 1.0306, time 109.29ms, mfu 3.22%\n",
            "iter 2860: loss 1.0237, time 111.06ms, mfu 3.24%\n",
            "iter 2870: loss 1.0016, time 111.72ms, mfu 3.25%\n",
            "iter 2880: loss 1.0270, time 112.13ms, mfu 3.26%\n",
            "iter 2890: loss 1.0150, time 111.95ms, mfu 3.26%\n",
            "iter 2900: loss 0.9963, time 113.97ms, mfu 3.26%\n",
            "iter 2910: loss 1.0427, time 111.48ms, mfu 3.27%\n",
            "iter 2920: loss 1.0190, time 110.77ms, mfu 3.28%\n",
            "iter 2930: loss 0.9971, time 112.12ms, mfu 3.28%\n",
            "iter 2940: loss 0.9860, time 111.25ms, mfu 3.29%\n",
            "iter 2950: loss 1.0231, time 110.44ms, mfu 3.30%\n",
            "iter 2960: loss 1.0000, time 110.58ms, mfu 3.31%\n",
            "iter 2970: loss 0.9940, time 111.89ms, mfu 3.31%\n",
            "iter 2980: loss 1.0014, time 112.41ms, mfu 3.31%\n",
            "iter 2990: loss 0.9899, time 112.29ms, mfu 3.31%\n",
            "step 3000: train loss 0.8673, val loss 1.5194\n",
            "iter 3000: loss 0.9866, time 11446.52ms, mfu 2.98%\n",
            "iter 3010: loss 0.9940, time 112.18ms, mfu 3.02%\n",
            "iter 3020: loss 0.9946, time 112.57ms, mfu 3.05%\n",
            "iter 3030: loss 1.0067, time 122.50ms, mfu 3.05%\n",
            "iter 3040: loss 1.0279, time 112.04ms, mfu 3.07%\n",
            "iter 3050: loss 0.9855, time 111.64ms, mfu 3.10%\n",
            "iter 3060: loss 0.9980, time 111.81ms, mfu 3.12%\n",
            "iter 3070: loss 1.0166, time 110.53ms, mfu 3.15%\n",
            "iter 3080: loss 1.0064, time 110.86ms, mfu 3.17%\n",
            "iter 3090: loss 0.9795, time 110.87ms, mfu 3.19%\n",
            "iter 3100: loss 0.9986, time 112.35ms, mfu 3.20%\n",
            "iter 3110: loss 0.9806, time 112.31ms, mfu 3.21%\n",
            "iter 3120: loss 0.9921, time 111.82ms, mfu 3.22%\n",
            "iter 3130: loss 0.9779, time 112.55ms, mfu 3.23%\n",
            "iter 3140: loss 0.9753, time 111.06ms, mfu 3.25%\n",
            "iter 3150: loss 0.9882, time 110.73ms, mfu 3.26%\n",
            "iter 3160: loss 1.0155, time 111.52ms, mfu 3.27%\n",
            "iter 3170: loss 0.9620, time 111.35ms, mfu 3.27%\n",
            "iter 3180: loss 0.9784, time 110.44ms, mfu 3.28%\n",
            "iter 3190: loss 0.9972, time 114.02ms, mfu 3.28%\n",
            "iter 3200: loss 0.9690, time 110.73ms, mfu 3.29%\n",
            "iter 3210: loss 0.9701, time 111.73ms, mfu 3.30%\n",
            "iter 3220: loss 0.9613, time 112.75ms, mfu 3.30%\n",
            "iter 3230: loss 0.9596, time 112.33ms, mfu 3.30%\n",
            "iter 3240: loss 0.9608, time 112.85ms, mfu 3.30%\n",
            "step 3250: train loss 0.8239, val loss 1.5611\n",
            "iter 3250: loss 0.9846, time 11447.15ms, mfu 2.97%\n",
            "iter 3260: loss 0.9668, time 111.89ms, mfu 3.01%\n",
            "iter 3270: loss 0.9772, time 110.72ms, mfu 3.04%\n",
            "iter 3280: loss 0.9445, time 112.34ms, mfu 3.07%\n",
            "iter 3290: loss 0.9432, time 112.48ms, mfu 3.10%\n",
            "iter 3300: loss 0.9451, time 109.81ms, mfu 3.12%\n",
            "iter 3310: loss 0.9571, time 112.58ms, mfu 3.14%\n",
            "iter 3320: loss 0.9727, time 113.09ms, mfu 3.16%\n",
            "iter 3330: loss 0.9581, time 112.27ms, mfu 3.17%\n",
            "iter 3340: loss 0.9502, time 112.15ms, mfu 3.19%\n",
            "iter 3350: loss 0.9538, time 110.72ms, mfu 3.21%\n",
            "iter 3360: loss 0.9362, time 112.02ms, mfu 3.22%\n",
            "iter 3370: loss 0.9606, time 112.07ms, mfu 3.23%\n",
            "iter 3380: loss 0.9514, time 112.21ms, mfu 3.24%\n",
            "iter 3390: loss 0.9561, time 110.83ms, mfu 3.25%\n",
            "iter 3400: loss 0.9596, time 112.29ms, mfu 3.26%\n",
            "iter 3410: loss 0.9444, time 112.00ms, mfu 3.26%\n",
            "iter 3420: loss 0.9479, time 112.86ms, mfu 3.27%\n",
            "iter 3430: loss 0.9430, time 111.22ms, mfu 3.28%\n",
            "iter 3440: loss 0.9693, time 111.58ms, mfu 3.28%\n",
            "iter 3450: loss 0.9509, time 113.18ms, mfu 3.28%\n",
            "iter 3460: loss 0.9453, time 111.31ms, mfu 3.29%\n",
            "iter 3470: loss 0.9385, time 111.39ms, mfu 3.30%\n",
            "iter 3480: loss 0.9504, time 111.26ms, mfu 3.30%\n",
            "iter 3490: loss 0.9106, time 110.80ms, mfu 3.31%\n",
            "step 3500: train loss 0.7790, val loss 1.5720\n",
            "iter 3500: loss 0.9011, time 11359.67ms, mfu 2.98%\n",
            "iter 3510: loss 0.9180, time 110.31ms, mfu 3.02%\n",
            "iter 3520: loss 0.9251, time 113.87ms, mfu 3.04%\n",
            "iter 3530: loss 0.9584, time 111.04ms, mfu 3.08%\n",
            "iter 3540: loss 0.9311, time 111.45ms, mfu 3.10%\n",
            "iter 3550: loss 0.9215, time 111.55ms, mfu 3.13%\n",
            "iter 3560: loss 0.9519, time 112.25ms, mfu 3.15%\n",
            "iter 3570: loss 0.9363, time 112.02ms, mfu 3.16%\n",
            "iter 3580: loss 0.9419, time 109.95ms, mfu 3.19%\n",
            "iter 3590: loss 0.9228, time 112.60ms, mfu 3.20%\n",
            "iter 3600: loss 0.9237, time 110.82ms, mfu 3.22%\n",
            "iter 3610: loss 0.9118, time 111.70ms, mfu 3.23%\n",
            "iter 3620: loss 0.9171, time 111.22ms, mfu 3.24%\n",
            "iter 3630: loss 0.9199, time 110.66ms, mfu 3.25%\n",
            "iter 3640: loss 0.9228, time 111.07ms, mfu 3.26%\n",
            "iter 3650: loss 0.9075, time 112.85ms, mfu 3.27%\n",
            "iter 3660: loss 0.9391, time 111.69ms, mfu 3.27%\n",
            "iter 3670: loss 0.9375, time 110.65ms, mfu 3.28%\n",
            "iter 3680: loss 0.9084, time 112.72ms, mfu 3.29%\n",
            "iter 3690: loss 0.9350, time 110.08ms, mfu 3.30%\n",
            "iter 3700: loss 0.8690, time 113.02ms, mfu 3.30%\n",
            "iter 3710: loss 0.8807, time 110.12ms, mfu 3.30%\n",
            "iter 3720: loss 0.9150, time 111.64ms, mfu 3.31%\n",
            "iter 3730: loss 0.9002, time 110.48ms, mfu 3.31%\n",
            "iter 3740: loss 0.9056, time 111.57ms, mfu 3.32%\n",
            "step 3750: train loss 0.7414, val loss 1.6029\n",
            "iter 3750: loss 0.9066, time 11424.23ms, mfu 2.99%\n",
            "iter 3760: loss 0.9350, time 111.64ms, mfu 3.02%\n",
            "iter 3770: loss 0.9320, time 112.21ms, mfu 3.05%\n",
            "iter 3780: loss 0.9233, time 111.62ms, mfu 3.08%\n",
            "iter 3790: loss 0.9007, time 113.28ms, mfu 3.10%\n",
            "iter 3800: loss 0.9027, time 111.01ms, mfu 3.13%\n",
            "iter 3810: loss 0.9152, time 111.76ms, mfu 3.15%\n",
            "iter 3820: loss 0.8884, time 113.50ms, mfu 3.16%\n",
            "iter 3830: loss 0.9023, time 112.25ms, mfu 3.18%\n",
            "iter 3840: loss 0.8860, time 111.08ms, mfu 3.20%\n",
            "iter 3850: loss 0.8922, time 111.73ms, mfu 3.21%\n",
            "iter 3860: loss 0.8694, time 111.25ms, mfu 3.22%\n",
            "iter 3870: loss 0.8979, time 111.42ms, mfu 3.24%\n",
            "iter 3880: loss 0.8883, time 112.13ms, mfu 3.24%\n",
            "iter 3890: loss 0.8921, time 111.39ms, mfu 3.25%\n",
            "iter 3900: loss 0.8796, time 110.70ms, mfu 3.27%\n",
            "iter 3910: loss 0.8910, time 110.80ms, mfu 3.28%\n",
            "iter 3920: loss 0.8786, time 110.88ms, mfu 3.28%\n",
            "iter 3930: loss 0.8860, time 110.76ms, mfu 3.29%\n",
            "iter 3940: loss 0.8757, time 111.45ms, mfu 3.30%\n",
            "iter 3950: loss 0.8747, time 112.31ms, mfu 3.30%\n",
            "iter 3960: loss 0.9120, time 112.44ms, mfu 3.30%\n",
            "iter 3970: loss 0.8914, time 112.19ms, mfu 3.30%\n",
            "iter 3980: loss 0.9046, time 111.80ms, mfu 3.31%\n",
            "iter 3990: loss 0.8761, time 111.90ms, mfu 3.31%\n",
            "step 4000: train loss 0.7089, val loss 1.6181\n",
            "iter 4000: loss 0.8590, time 11440.01ms, mfu 2.98%\n",
            "iter 4010: loss 0.8836, time 112.44ms, mfu 3.01%\n",
            "iter 4020: loss 0.8819, time 111.78ms, mfu 3.05%\n",
            "iter 4030: loss 0.8776, time 112.79ms, mfu 3.07%\n",
            "iter 4040: loss 0.8846, time 111.56ms, mfu 3.10%\n",
            "iter 4050: loss 0.8734, time 110.93ms, mfu 3.12%\n",
            "iter 4060: loss 0.8648, time 112.09ms, mfu 3.14%\n",
            "iter 4070: loss 0.8631, time 111.04ms, mfu 3.17%\n",
            "iter 4080: loss 0.8867, time 110.09ms, mfu 3.19%\n",
            "iter 4090: loss 0.8479, time 111.27ms, mfu 3.20%\n",
            "iter 4100: loss 0.8987, time 111.69ms, mfu 3.22%\n",
            "iter 4110: loss 0.8641, time 112.30ms, mfu 3.23%\n",
            "iter 4120: loss 0.8797, time 112.46ms, mfu 3.24%\n",
            "iter 4130: loss 0.8548, time 110.32ms, mfu 3.25%\n",
            "iter 4140: loss 0.8778, time 111.77ms, mfu 3.26%\n",
            "iter 4150: loss 0.8723, time 112.32ms, mfu 3.26%\n",
            "iter 4160: loss 0.8512, time 111.92ms, mfu 3.27%\n",
            "iter 4170: loss 0.8705, time 109.42ms, mfu 3.28%\n",
            "iter 4180: loss 0.8739, time 110.84ms, mfu 3.29%\n",
            "iter 4190: loss 0.8654, time 113.25ms, mfu 3.29%\n",
            "iter 4200: loss 0.8579, time 110.55ms, mfu 3.30%\n",
            "iter 4210: loss 0.8754, time 110.26ms, mfu 3.31%\n",
            "iter 4220: loss 0.8595, time 111.29ms, mfu 3.31%\n",
            "iter 4230: loss 0.8805, time 111.51ms, mfu 3.31%\n",
            "iter 4240: loss 0.8654, time 111.97ms, mfu 3.32%\n",
            "step 4250: train loss 0.6784, val loss 1.6442\n",
            "iter 4250: loss 0.8753, time 11357.40ms, mfu 2.99%\n",
            "iter 4260: loss 0.8598, time 111.08ms, mfu 3.02%\n",
            "iter 4270: loss 0.8634, time 112.07ms, mfu 3.05%\n",
            "iter 4280: loss 0.8539, time 111.97ms, mfu 3.08%\n",
            "iter 4290: loss 0.8318, time 112.00ms, mfu 3.11%\n",
            "iter 4300: loss 0.8307, time 110.70ms, mfu 3.13%\n",
            "iter 4310: loss 0.8535, time 110.87ms, mfu 3.16%\n",
            "iter 4320: loss 0.8466, time 110.98ms, mfu 3.18%\n",
            "iter 4330: loss 0.8621, time 110.79ms, mfu 3.19%\n",
            "iter 4340: loss 0.8286, time 110.99ms, mfu 3.21%\n",
            "iter 4350: loss 0.8388, time 110.37ms, mfu 3.23%\n",
            "iter 4360: loss 0.8612, time 112.98ms, mfu 3.23%\n",
            "iter 4370: loss 0.8579, time 112.15ms, mfu 3.24%\n",
            "iter 4380: loss 0.8347, time 112.12ms, mfu 3.25%\n",
            "iter 4390: loss 0.8672, time 112.01ms, mfu 3.26%\n",
            "iter 4400: loss 0.8440, time 114.14ms, mfu 3.26%\n",
            "iter 4410: loss 0.8630, time 108.31ms, mfu 3.28%\n",
            "iter 4420: loss 0.8611, time 111.49ms, mfu 3.28%\n",
            "iter 4430: loss 0.8399, time 109.68ms, mfu 3.30%\n",
            "iter 4440: loss 0.8514, time 109.57ms, mfu 3.31%\n",
            "iter 4450: loss 0.8549, time 113.54ms, mfu 3.30%\n",
            "iter 4460: loss 0.8366, time 113.00ms, mfu 3.30%\n",
            "iter 4470: loss 0.8530, time 113.21ms, mfu 3.30%\n",
            "iter 4480: loss 0.8332, time 112.17ms, mfu 3.30%\n",
            "iter 4490: loss 0.8419, time 112.19ms, mfu 3.31%\n",
            "step 4500: train loss 0.6544, val loss 1.6613\n",
            "iter 4500: loss 0.8533, time 11480.93ms, mfu 2.98%\n",
            "iter 4510: loss 0.8460, time 111.13ms, mfu 3.02%\n",
            "iter 4520: loss 0.8332, time 111.61ms, mfu 3.05%\n",
            "iter 4530: loss 0.8519, time 112.45ms, mfu 3.07%\n",
            "iter 4540: loss 0.8431, time 110.81ms, mfu 3.10%\n",
            "iter 4550: loss 0.8768, time 111.82ms, mfu 3.13%\n",
            "iter 4560: loss 0.8361, time 111.97ms, mfu 3.15%\n",
            "iter 4570: loss 0.8384, time 112.03ms, mfu 3.16%\n",
            "iter 4580: loss 0.8528, time 113.22ms, mfu 3.18%\n",
            "iter 4590: loss 0.8554, time 110.57ms, mfu 3.20%\n",
            "iter 4600: loss 0.8282, time 111.21ms, mfu 3.21%\n",
            "iter 4610: loss 0.8602, time 112.31ms, mfu 3.22%\n",
            "iter 4620: loss 0.8341, time 111.71ms, mfu 3.23%\n",
            "iter 4630: loss 0.8136, time 110.30ms, mfu 3.25%\n",
            "iter 4640: loss 0.8465, time 112.18ms, mfu 3.26%\n",
            "iter 4650: loss 0.8606, time 111.15ms, mfu 3.27%\n",
            "iter 4660: loss 0.8533, time 111.70ms, mfu 3.27%\n",
            "iter 4670: loss 0.8358, time 112.13ms, mfu 3.28%\n",
            "iter 4680: loss 0.8533, time 111.89ms, mfu 3.28%\n",
            "iter 4690: loss 0.8411, time 110.62ms, mfu 3.29%\n",
            "iter 4700: loss 0.8206, time 112.70ms, mfu 3.29%\n",
            "iter 4710: loss 0.7934, time 113.56ms, mfu 3.29%\n",
            "iter 4720: loss 0.8374, time 111.59ms, mfu 3.30%\n",
            "iter 4730: loss 0.8223, time 112.76ms, mfu 3.30%\n",
            "iter 4740: loss 0.8268, time 111.86ms, mfu 3.30%\n",
            "step 4750: train loss 0.6359, val loss 1.6802\n",
            "iter 4750: loss 0.8078, time 11348.07ms, mfu 2.97%\n",
            "iter 4760: loss 0.8218, time 111.73ms, mfu 3.01%\n",
            "iter 4770: loss 0.8055, time 111.91ms, mfu 3.04%\n",
            "iter 4780: loss 0.8098, time 110.91ms, mfu 3.07%\n",
            "iter 4790: loss 0.8343, time 109.49ms, mfu 3.11%\n",
            "iter 4800: loss 0.8281, time 110.76ms, mfu 3.13%\n",
            "iter 4810: loss 0.8343, time 109.60ms, mfu 3.16%\n",
            "iter 4820: loss 0.8217, time 112.05ms, mfu 3.18%\n",
            "iter 4830: loss 0.8183, time 112.42ms, mfu 3.19%\n",
            "iter 4840: loss 0.8372, time 110.60ms, mfu 3.21%\n",
            "iter 4850: loss 0.8202, time 109.33ms, mfu 3.23%\n",
            "iter 4860: loss 0.8241, time 113.18ms, mfu 3.23%\n",
            "iter 4870: loss 0.8103, time 113.03ms, mfu 3.24%\n",
            "iter 4880: loss 0.8312, time 111.20ms, mfu 3.25%\n",
            "iter 4890: loss 0.8079, time 112.83ms, mfu 3.26%\n",
            "iter 4900: loss 0.8054, time 114.99ms, mfu 3.25%\n",
            "iter 4910: loss 0.8333, time 114.20ms, mfu 3.26%\n",
            "iter 4920: loss 0.8248, time 111.49ms, mfu 3.26%\n",
            "iter 4930: loss 0.8068, time 112.56ms, mfu 3.27%\n",
            "iter 4940: loss 0.8003, time 113.97ms, mfu 3.27%\n",
            "iter 4950: loss 0.8276, time 109.28ms, mfu 3.28%\n",
            "iter 4960: loss 0.8309, time 111.23ms, mfu 3.29%\n",
            "iter 4970: loss 0.7909, time 111.74ms, mfu 3.29%\n",
            "iter 4980: loss 0.7935, time 111.00ms, mfu 3.30%\n",
            "iter 4990: loss 0.8205, time 113.00ms, mfu 3.30%\n",
            "step 5000: train loss 0.6210, val loss 1.7005\n",
            "iter 5000: loss 0.8196, time 11469.12ms, mfu 2.97%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the model"
      ],
      "metadata": {
        "id": "5-2uBNuQbkzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python sample.py --out_dir=out-shakespeare-char"
      ],
      "metadata": {
        "id": "Q5fLCijpXP3f",
        "outputId": "ee5949d0-aff4-47eb-f4a6-d48e13a9f95f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-shakespeare-char\n",
            "number of parameters: 10.65M\n",
            "Loading meta from data/shakespeare_char/meta.pkl...\n",
            "\n",
            "\n",
            "Clown:\n",
            "So you will be a fellow a servant. He have not strange?\n",
            "\n",
            "MONTAGUE:\n",
            "No more, my lord: he's never court.\n",
            "\n",
            "ANGELO:\n",
            "Go on,\n",
            "You shall be a common that little might show.\n",
            "\n",
            "ANGELO:\n",
            "Alack to the queen.\n",
            "\n",
            "ANGELO:\n",
            "Here's no more.\n",
            "\n",
            "ANGELO:\n",
            "But you are near like for this. Go with this good\n",
            "to sweet the king Willoughby princely.\n",
            "\n",
            "ANGELO:\n",
            "Even she is the common of the king thrust for the\n",
            "great of this isle of the field?\n",
            "\n",
            "ISABELLA:\n",
            "I shall to be a word: but it is so.\n",
            "\n",
            "ISABELLA:\n",
            "I tell you, my lord; I wi\n",
            "---------------\n",
            "\n",
            "MenEnius, I must thou continue me\n",
            "That I have sent forth and late.\n",
            "\n",
            "CORIOLANUS:\n",
            "Change thee,\n",
            "One that frowns should sold. A silly thing it\n",
            "To see thee. Thou art a maid?\n",
            "\n",
            "LARTIUS:\n",
            "My very time:\n",
            "Here's a Volscian, sir, come, sir, sir, thou art a very root\n",
            "To an unballable: 'tis no world of this selfname\n",
            "Whose lasts our voices' shore, his heir\n",
            "Than he nothing Camillo is the office\n",
            "To her sent a submissips; and will not so the silk\n",
            "Even with the crown, to prove a watch, if\n",
            "I can rest; and think it e\n",
            "---------------\n",
            "\n",
            "MARIANA:\n",
            "I beseech you, I would weep.\n",
            "My lord, I do dinner and I would you bring you.\n",
            "\n",
            "ROMEO:\n",
            "It is a fellow till your sons, growing as\n",
            "I dream'd your gracious senIUS:\n",
            "You take to the covert and in this blame\n",
            "In my breath in men man is walling out of it;\n",
            "And still you with that with the prisoner.\n",
            "\n",
            "Nurse:\n",
            "What does not?\n",
            "Madam? O my heart with you? how I was,\n",
            "Provost, 'tis quickly have been dead!'\n",
            "\n",
            "LADY CAPULET:\n",
            "Well, that I have quite to the town throughese man\n",
            "I' the instrument, so much of thee\n",
            "\n",
            "---------------\n",
            "\n",
            "The straight will keep the sweetest princes for kove,\n",
            "As every top the officers, who was straight\n",
            "To score the shame, and patricians.\n",
            "\n",
            "BENVOLIO:\n",
            "I am worn. The lark of Rome, Romeo comes\n",
            "Shall bear the news.\n",
            "\n",
            "MERCUTIO:\n",
            "O her love! her wife, thou with him:\n",
            "Here's companion, Lucio.\n",
            "\n",
            "MERCUTIO:\n",
            "An if you be fair?\n",
            "\n",
            "BENVOLIO:\n",
            "The rest, are no like a sound fool.\n",
            "\n",
            "MERCUTIO:\n",
            "O, true, thou hast dead! most dead, an old transport\n",
            "Than do thou threat parliaments!\n",
            "\n",
            "ROMEO:\n",
            "I fear, how she was ever briever and h\n",
            "---------------\n",
            "\n",
            "BUCKINGHAM:\n",
            "Then, here is not some stinlight for your grace,\n",
            "Not yet ever so much every are there\n",
            "But in the people's parliament. Some in the man\n",
            "Being moody to swimit it of you.\n",
            "\n",
            "BUCKINGHAM:\n",
            "No other more come to make his most noble great;\n",
            "And, by her name well appear the nounted soldiers,\n",
            "Being common your worship not and under his soundly.\n",
            "\n",
            "GLOUCESTER:\n",
            "Here comes her than the tedious course.\n",
            "\n",
            "KING EDWARD IV:\n",
            "And then moves so he that would as dead.\n",
            "Go on, and will keep our grace; and then for\n",
            "---------------\n",
            "\n",
            "\n",
            "MENENIUS:\n",
            "He's a letter for Corioli: he's\n",
            "a gracious lord.\n",
            "\n",
            "First Citizen:\n",
            "Away, away!\n",
            "\n",
            "MENENIUS:\n",
            "I had a lack of the people, the\n",
            "people! Take him dead, I had said to chance.\n",
            "\n",
            "CORIOLANUS:\n",
            "What's change to the people? have you not\n",
            "to have an abroad with the seeming noise the proud?\n",
            "\n",
            "Second Citizen:\n",
            "Conceit it, Backingham, that walk appointed in the pride.\n",
            "\n",
            "CORIOLANUS:\n",
            "I cannot think with all the tomb, that he does\n",
            "The vilgary of mine and his bed,\n",
            "Her disgraced him, dear him, to do weep with the \n",
            "---------------\n",
            "\n",
            "Shepherd:\n",
            "Heaven if so, steel it not.\n",
            "\n",
            "Clown:\n",
            "I have not to many of this all, and whose\n",
            "the shores: your blestings are bried intents\n",
            "none world, and have you so?\n",
            "\n",
            "Clown:\n",
            "Will you not banishment, that you have deserved them, who\n",
            "seen you do, sir, which I must conto any fool, a\n",
            "rashning from this is little bustrains. Your\n",
            "wisdom to see alone all. Proceeds to the limb; a while I woo do it alone.\n",
            "\n",
            "Clown:\n",
            "My lord, hold I tell you, and I would prophesy to\n",
            "hold that which you were look not like a fool.\n",
            "---------------\n",
            "\n",
            "ISABELLA:\n",
            "True, if not me, or with mine honour,\n",
            "I do not banish you with her rest,\n",
            "That I should not hear me with her eyes,\n",
            "The world is beholding in enterial.\n",
            "\n",
            "ANGELO:\n",
            "The battle hours I do disgrace the sword\n",
            "Of her tongue, and to such a brother world in earth!\n",
            "\n",
            "ISABELLA:\n",
            "Now, sir, if it breathed not, I will tell you fry them?\n",
            "\n",
            "ANGELO:\n",
            "See you, tell me this winter'd divines.\n",
            "\n",
            "ISABELLA:\n",
            "How doth here this, my lord; but it is an abuse\n",
            "Within this approbation, and she was much a maid\n",
            "But in the ol\n",
            "---------------\n",
            "\n",
            "GLOUCESTER:\n",
            "Lords, I can not see the other sad:\n",
            "And with her, which she shall seem for their swordship.\n",
            "\n",
            "WARWICK:\n",
            "Bloody maids the feast.\n",
            "\n",
            "KING LEWIS XI:\n",
            "My Lord of Gloucester, and will thither.\n",
            "\n",
            "WARWICK:\n",
            "No, none, some wish her and reverenge.\n",
            "\n",
            "YORK:\n",
            "To accompanion, we be too homed not.\n",
            "\n",
            "DERBY:\n",
            "She that is not for a fool's death;\n",
            "But let me hence there shall forget:\n",
            "And in her father's pure be woo'd up to be so grown,\n",
            "Even her brief in the harmless upregnate:\n",
            "I then she comes much with the beast\n",
            "---------------\n",
            "\n",
            "How changed the drops of eight of his soul?\n",
            "\n",
            "ISABELLA:\n",
            "This is the day of this is your land;\n",
            "But I have been call'd up him been your tent?\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "How far of the solemnity? who is wrong'd?\n",
            "Why should we shame an arms stoop of life?\n",
            "They will prove his like offence with life\n",
            "And to be crave a happy model's guilty of his cheeks;\n",
            "For all his foes, that are gone of me.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Their love and press sent to it.\n",
            "\n",
            "CLAUDIO:\n",
            "Brother is a visitor.\n",
            "\n",
            "ANGELO:\n",
            "Why, 'tis my lord, I die the Cap\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ESIG8GQNf3p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Yay! Good results!\n",
        "#### Of course la, cause we just copied Karpathy's code"
      ],
      "metadata": {
        "id": "AErRUfZ-fpwQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "FMMSTGE_f2vP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time to Get Down in the Trenches!!"
      ],
      "metadata": {
        "id": "cotKVDYsfNZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import requests\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "O8tBDeKogXNF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Data"
      ],
      "metadata": {
        "id": "HnW3qGZegEPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__file__ = '/content/nanoGPT/data/shakespeare_char'"
      ],
      "metadata": {
        "id": "ukQrqDb8gtYx"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download the tiny shakespeare dataset\n",
        "input_file_path = os.path.join(os.path.dirname(__file__), 'input.txt')\n",
        "if not os.path.exists(input_file_path):\n",
        "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "    with open(input_file_path, 'w') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "\n",
        "with open(input_file_path, 'r') as f:\n",
        "    data = f.read()\n",
        "print(f\"length of dataset in characters: {len(data):,}\")"
      ],
      "metadata": {
        "id": "wioUsMnhgDxZ",
        "outputId": "9ceafb1a-74d7-4a67-cc33-14071bc1bfa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get all the unique characters that occur in this text\n",
        "chars = sorted(list(set(data)))\n",
        "vocab_size = len(chars)\n",
        "print(\"all the unique characters:\", ''.join(chars))\n",
        "print(f\"vocab size: {vocab_size:,}\")"
      ],
      "metadata": {
        "id": "ndQIrpY9XPyQ",
        "outputId": "6e16fb87-89e7-45ce-e9b3-3063c7a3c8a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "def encode(s):\n",
        "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "def decode(l):\n",
        "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n"
      ],
      "metadata": {
        "id": "bd7XrHdAXPvn"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the train and test splits\n",
        "n = len(data)\n",
        "train_data = data[:int(n*0.9)]\n",
        "val_data = data[int(n*0.9):]"
      ],
      "metadata": {
        "id": "O3gfOb0zXPtB"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encode both to integers\n",
        "train_ids = encode(train_data)\n",
        "val_ids = encode(val_data)\n",
        "print(f\"train has {len(train_ids):,} tokens\")\n",
        "print(f\"val has {len(val_ids):,} tokens\")\n",
        "\n"
      ],
      "metadata": {
        "id": "N1G-eXnzXPqU",
        "outputId": "99f21449-0a23-4a6b-e7e7-f16b59d33836",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# export to bin files\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "train_ids.tofile(os.path.join(os.path.dirname(__file__), 'train.bin'))\n",
        "val_ids.tofile(os.path.join(os.path.dirname(__file__), 'val.bin'))\n",
        "\n"
      ],
      "metadata": {
        "id": "uggVaBh_XPny"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the meta information as well, to help us encode/decode later\n",
        "meta = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'itos': itos,\n",
        "    'stoi': stoi,\n",
        "}\n",
        "with open(os.path.join(os.path.dirname(__file__), 'meta.pkl'), 'wb') as f:\n",
        "    pickle.dump(meta, f)"
      ],
      "metadata": {
        "id": "vZIS_HXtXPlM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "GzkbFyH50INM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "\n",
        "from model import GPTConfig, GPT"
      ],
      "metadata": {
        "id": "555GzN6LXPis"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
        "# I/O\n",
        "out_dir = 'out'\n",
        "eval_interval = 2000\n",
        "log_interval = 1\n",
        "eval_iters = 200\n",
        "eval_only = False # if True, script exits right after the first eval\n",
        "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
        "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
        "# wandb logging\n",
        "wandb_log = False # disabled by default\n",
        "wandb_project = 'owt'\n",
        "wandb_run_name = 'gpt2' # 'run' + str(time.time())\n",
        "# data\n",
        "dataset = 'openwebtext'\n",
        "gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes\n",
        "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
        "block_size = 1024\n",
        "# model\n",
        "n_layer = 12\n",
        "n_head = 12\n",
        "n_embd = 768\n",
        "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
        "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
        "# adamw optimizer\n",
        "learning_rate = 6e-4 # max learning rate\n",
        "max_iters = 600000 # total number of training iterations\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
        "# learning rate decay settings\n",
        "decay_lr = True # whether to decay the learning rate\n",
        "warmup_iters = 2000 # how many steps to warm up for\n",
        "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
        "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
        "# DDP settings\n",
        "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
        "# system\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
        "# -----------------------------------------------------------------------------\n",
        "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
        "exec(open('configurator.py').read()) # overrides from command line or config file\n",
        "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
        "# -----------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "-fdpD4SG1FvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# various inits, derived attributes, I/O setup\n",
        "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
        "if ddp:\n",
        "    init_process_group(backend=backend)\n",
        "    ddp_rank = int(os.environ['RANK'])\n",
        "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "    device = f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
        "    seed_offset = ddp_rank # each process gets a different seed\n",
        "    # world_size number of processes will be training simultaneously, so we can scale\n",
        "    # down the desired gradient accumulation iterations per process proportionally\n",
        "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
        "    gradient_accumulation_steps //= ddp_world_size\n",
        "else:\n",
        "    # if not ddp, we are running on a single gpu, and one process\n",
        "    master_process = True\n",
        "    seed_offset = 0\n",
        "    ddp_world_size = 1\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "if master_process:\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ],
      "metadata": {
        "id": "uRWNJ4rgCkGd",
        "outputId": "240da5d8-46ce-48fe-b1ba-91e472113be4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens per iteration will be: 491,520\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ''\n",
        "# poor man's data loader\n",
        "data_dir = os.path.join('data', dataset)\n",
        "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "DswjDsC0XPgI"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "\n",
        "# attempt to derive vocab_size from the dataset\n",
        "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
        "meta_vocab_size = None\n",
        "if os.path.exists(meta_path):\n",
        "    with open(meta_path, 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "    meta_vocab_size = meta['vocab_size']\n",
        "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n"
      ],
      "metadata": {
        "id": "8rm7CftDXPdw",
        "outputId": "732da6b1-c8ed-4adc-b7fe-67b7578a09b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found vocab_size = 65 (inside data/meta.pkl)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model init\n",
        "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
        "                  bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line"
      ],
      "metadata": {
        "id": "AecYqGOlXPbU"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # init a new model from scratch\n",
        "  print(\"Initializing a new model from scratch\")\n",
        "  # determine the vocab size we'll use for from-scratch training\n",
        "  if meta_vocab_size is None:\n",
        "      print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
        "  model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
        "  gptconf = GPTConfig(**model_args)\n",
        "  model = GPT(gptconf)"
      ],
      "metadata": {
        "id": "UQOqGrWz8Bdu",
        "outputId": "1da1715f-4f20-4683-f243-604679229dac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing a new model from scratch\n",
            "number of parameters: 85.00M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# crop down the model block size if desired, using model surgery\n",
        "if block_size < model.config.block_size:\n",
        "    model.crop_block_size(block_size)\n",
        "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
        "model.to(device)\n",
        "\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "\n",
        "# optimizer\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "\n",
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0"
      ],
      "metadata": {
        "id": "3xtxQ8Es8Vql",
        "outputId": "d49e4c1e-d130-4594-eb1d-b0728a67ca79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 50, with 85,771,008 parameters\n",
            "num non-decayed parameter tensors: 25, with 19,200 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n"
      ],
      "metadata": {
        "id": "wZQ4QoM38eAY"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# training loop\n",
        "X, Y = get_batch('train') # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
        "running_mfu = -1.0\n",
        "while True:\n",
        "\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if wandb_log:\n",
        "            wandb.log({\n",
        "                \"iter\": iter_num,\n",
        "                \"train/loss\": losses['train'],\n",
        "                \"val/loss\": losses['val'],\n",
        "                \"lr\": lr,\n",
        "                \"mfu\": running_mfu*100, # convert to percentage\n",
        "            })\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'config': config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        if ddp:\n",
        "            # in DDP training we only need to sync gradients at the last micro step.\n",
        "            # the official way to do this is with model.no_sync() context manager, but\n",
        "            # I really dislike that this bloats the code and forces us to repeat code\n",
        "            # looking at the source of that context manager, it just toggles this variable\n",
        "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        # get loss as float. note: this is a CPU-GPU sync point\n",
        "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()"
      ],
      "metadata": {
        "id": "rEtsDa1u8vub",
        "outputId": "d3adfbdf-a6da-4cfb-811b-9a1f2a1150a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.4501, val loss 4.4401\n",
            "iter 0: loss 4.4554, time 94874.03ms, mfu -100.00%\n",
            "iter 1: loss 4.4650, time 18694.50ms, mfu -100.00%\n",
            "iter 2: loss 4.4035, time 18804.22ms, mfu -100.00%\n",
            "iter 3: loss 4.3096, time 18207.60ms, mfu -100.00%\n",
            "iter 4: loss 4.1738, time 17985.40ms, mfu -100.00%\n",
            "iter 5: loss 4.0113, time 18134.14ms, mfu 5.41%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/content/nanoGPT/data/shakespeare_char\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# backward pass, with gradient scaling if training in fp16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;31m# clip the gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgrad_clip\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VpPx7neMXPY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6K80PC3JXPWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IteKaBCFyYEO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}